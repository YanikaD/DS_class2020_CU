{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1-final"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5yJiZaen0fH"
      },
      "source": [
        "# Assignment - Basic Pandas\n",
        "<sup>Created by Natawut Nupairoj, Department of Computer Engineering, Chulalongkorn University</sup>\n",
        "\n",
        "Using pandas to explore youtube trending data from GB (GBvideos.csv and GB_category_id.json) and answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4R5pu-Xn0fJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD6uPALan0fL"
      },
      "source": [
        "To simplify data retrieval process on Colab, we heck if we are in the Colab environment and download data files from a shared drive and save them in folder \"data\".\n",
        "\n",
        "For those using jupyter notebook on the local computer, you can read data directly assuming you save data in the folder \"data\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec4-TthIn0fM",
        "outputId": "a67e2b1c-075a-411a-ecb4-36e47136d247"
      },
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    !wget https://www.dropbox.com/s/7u5j5uei57yyksh/data.tgz?dl=0 -O data.tgz\n",
        "    !tar -xzvf data.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-29 14:19:11--  https://www.dropbox.com/s/7u5j5uei57yyksh/data.tgz?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/7u5j5uei57yyksh/data.tgz [following]\n",
            "--2021-01-29 14:19:11--  https://www.dropbox.com/s/raw/7u5j5uei57yyksh/data.tgz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc51a46885950af63e09a3766bc0.dl.dropboxusercontent.com/cd/0/inline/BH6PX1D3GpRTDDnKussDN9nthpRffCnhlVs-o3AL2u0N0HUwOXPzB7LjNnbCOe9wWOUni202sHnTvhakkl56IFxWZlIyGHMp9a_2MtjlZ-ruSMGi6Oy42LxQFaHuETItK7o/file# [following]\n",
            "--2021-01-29 14:19:12--  https://uc51a46885950af63e09a3766bc0.dl.dropboxusercontent.com/cd/0/inline/BH6PX1D3GpRTDDnKussDN9nthpRffCnhlVs-o3AL2u0N0HUwOXPzB7LjNnbCOe9wWOUni202sHnTvhakkl56IFxWZlIyGHMp9a_2MtjlZ-ruSMGi6Oy42LxQFaHuETItK7o/file\n",
            "Resolving uc51a46885950af63e09a3766bc0.dl.dropboxusercontent.com (uc51a46885950af63e09a3766bc0.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc51a46885950af63e09a3766bc0.dl.dropboxusercontent.com (uc51a46885950af63e09a3766bc0.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BH6r_bkWt5AQx2QtrXCPCIS1K4UGe16Nx-PgcIKqTzS2rRshl2hFtwXI6iqqKx7JLJ8QP0SH1KGPrPeUdrd4XG3K9RdydZtE6jFhYj4UBtOXJwuF_rrFB-vsAnL9kIAGBVfQBS3_WViTbkg6KCbJRpOHLiGSIKrVVD7RfwHqjJG_ouWXdD06hB2Ihn59LBs32cbI26TfxzH6-1ZF9fd2bR023IPfg1bbGKo3C39N-g85DoHNR0Dj5kmj4g-ZHvoXv7ZXvMXX4n9RNSoxASrirAWtXnT26bx1TA2_1UmP_MKGpCcqWg_BpEuNcBZWnsN9IaCVgmAtFQUtoEf0dw8RJpkVKEA2gRcY8s4ME-vv5qnYTA/file [following]\n",
            "--2021-01-29 14:19:12--  https://uc51a46885950af63e09a3766bc0.dl.dropboxusercontent.com/cd/0/inline2/BH6r_bkWt5AQx2QtrXCPCIS1K4UGe16Nx-PgcIKqTzS2rRshl2hFtwXI6iqqKx7JLJ8QP0SH1KGPrPeUdrd4XG3K9RdydZtE6jFhYj4UBtOXJwuF_rrFB-vsAnL9kIAGBVfQBS3_WViTbkg6KCbJRpOHLiGSIKrVVD7RfwHqjJG_ouWXdD06hB2Ihn59LBs32cbI26TfxzH6-1ZF9fd2bR023IPfg1bbGKo3C39N-g85DoHNR0Dj5kmj4g-ZHvoXv7ZXvMXX4n9RNSoxASrirAWtXnT26bx1TA2_1UmP_MKGpCcqWg_BpEuNcBZWnsN9IaCVgmAtFQUtoEf0dw8RJpkVKEA2gRcY8s4ME-vv5qnYTA/file\n",
            "Reusing existing connection to uc51a46885950af63e09a3766bc0.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45477462 (43M) [application/x-gtar]\n",
            "Saving to: ‘data.tgz’\n",
            "\n",
            "data.tgz            100%[===================>]  43.37M  47.7MB/s    in 0.9s    \n",
            "\n",
            "2021-01-29 14:19:14 (47.7 MB/s) - ‘data.tgz’ saved [45477462/45477462]\n",
            "\n",
            "data/\n",
            "data/._GB_category_id.json\n",
            "data/GB_category_id.json\n",
            "data/._US_category_id.json\n",
            "data/US_category_id.json\n",
            "data/._USvideos.csv\n",
            "data/USvideos.csv\n",
            "data/._GBvideos.csv\n",
            "data/GBvideos.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRF15a6gn0fN"
      },
      "source": [
        "## How many rows are there in the GBvideos.csv after removing duplications?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxdy8qEqn0fN",
        "outputId": "593e598b-0f25-46b2-cc1d-d302e1bde903"
      },
      "source": [
        "gb_vdo_df = pd.read_csv('data/GBvideos.csv')\n",
        "gb_vdo_df.drop_duplicates(inplace=True)\n",
        "gb_vdo_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38745, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h1UgctjoGKj"
      },
      "source": [
        "So, There are **38745** rows after removing duplications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKU7K8pmn0fP"
      },
      "source": [
        "## How many VDO that have \"dislikes\" more than \"likes\"?  Make sure that you count only unique title!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J2_Sbmwn0fP",
        "outputId": "15aa153d-413e-4e18-dde6-648d1b98df10"
      },
      "source": [
        "more_dislikes_vdo = gb_vdo_df[(gb_vdo_df.dislikes>gb_vdo_df.likes)]\r\n",
        "unique_title = more_dislikes_vdo.title.unique()\r\n",
        "len(unique_title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNewvFbdOwFo"
      },
      "source": [
        "So, There are **56** videos that have \"dislike\" more than \"likes\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ7fIEnbn0fP"
      },
      "source": [
        "## How many VDO that are trending on 22 Jan 2018 with comments more than 10,000 comments?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7fsktOjn0fP",
        "outputId": "cbbe945c-5b4a-47b3-820e-efb38c3fe965"
      },
      "source": [
        "gb_vdo_df['trending_dt'] = pd.to_datetime(gb_vdo_df.trending_date, format='%y.%d.%m', errors='ignore', utc=True)\r\n",
        "comment_more = gb_vdo_df[gb_vdo_df.comment_count>10000]\r\n",
        "gb_vdo_df_groupby_trending = comment_more.groupby('trending_dt') \r\n",
        "trending_22_jan_2018 = gb_vdo_df_groupby_trending.get_group('2018-01-22 00:00:00+00:00')\r\n",
        "len(trending_22_jan_2018)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-zBtdodRY9x"
      },
      "source": [
        "So, There are **28** videos that are trending on 22 Jan 2018 with comments more than 10,000 comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6E_QTCZn0fQ"
      },
      "source": [
        "## Which date that has the minimum average number of comments per VDO?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-IQrluBn0fR",
        "outputId": "f8a82005-87ca-4688-aa3a-68791d61f79d"
      },
      "source": [
        "avg_comment_each_date=gb_vdo_df_groupby_trending.mean()['comment_count']\r\n",
        "sum = gb_vdo_df_groupby_trending.sum()['comment_count']\r\n",
        "min_avg_comment_each_date = min(avg_comment_each_date)\r\n",
        "for date, group in gb_vdo_df_groupby_trending:\r\n",
        "  avg_group = group['comment_count'].mean()\r\n",
        "  if (min_avg_comment_each_date == avg_group):\r\n",
        "    print(\"date: \",date)\r\n",
        "    print(\"group average: \",avg_group)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date:  2017-11-14 00:00:00+00:00\n",
            "group average:  26409.115384615383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x7ewwm478aS"
      },
      "source": [
        "So, **14/11/2017** is the date that has minimum average number of comments per VDO which is around **26,409** comments per VDO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ikhJuKYn0fS"
      },
      "source": [
        "## Compare \"Sports\" and \"Comady\", how many days that there are more total daily views of VDO in \"Sports\" category than in \"Comady\" category?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLxQH5FLn0fS"
      },
      "source": [
        "import json\r\n",
        "with open('data/GB_category_id.json') as fd:\r\n",
        "  cat = json.load(fd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-q_Cskhg8Q4"
      },
      "source": [
        "cat_list = []\r\n",
        "for d in cat['items']:\r\n",
        "  cat_list.append((int(d['id']), d['snippet']['title']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMwqIC7jhKjs"
      },
      "source": [
        "cat_df = pd.DataFrame(cat_list, columns=['id', 'category'])\r\n",
        "gb_vdo_df_withcat = gb_vdo_df.merge(cat_df, left_on='category_id', right_on='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oykr-7FDh-u3",
        "outputId": "147cc898-2d8c-410a-dae8-e3dce177bc51"
      },
      "source": [
        "gb_vdo_df_withcat_grouby_date = gb_vdo_df_withcat.groupby([\"trending_dt\",\"category\"])\r\n",
        "date_sports_more_comedy = []\r\n",
        "sport_views = 0\r\n",
        "comedy_views =0\r\n",
        "date = \"\"\r\n",
        "for col, group in gb_vdo_df_withcat_grouby_date:\r\n",
        "  if col[0] == date and sport_views>comedy_views:\r\n",
        "      date_sports_more_comedy.append(date)\r\n",
        "  else:\r\n",
        "    if  col[1] == \"Sports\":\r\n",
        "      sport_views = float(group.views.sum())\r\n",
        "      date =col[0]\r\n",
        "    elif col[1] == \"Comedy\":\r\n",
        "      comedy_views =  float(group.views.sum())\r\n",
        "      date = col[0]\r\n",
        "\r\n",
        "len(date_sports_more_comedy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "780"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoUtqJNdzgFx"
      },
      "source": [
        "So, There are **780** days that Sports views is higher than Comedy views."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Sd0YSenx8o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}